{"cells":[{"cell_type":"markdown","source":["# Microsoft Azure automated ML Demo - v2"],"metadata":{}},{"cell_type":"markdown","source":["Azure ML & Azure Databricks notebooks by Parashar Shah.\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT License."],"metadata":{}},{"cell_type":"markdown","source":["## Purpose and Challenge"],"metadata":{}},{"cell_type":"markdown","source":["The purpose of this notebook is for the user to build and deploy a Machine Learning (ML) application using Azure Machine Learning (AML) service. It is a predictive maintenance scenario based on https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan.\n\nThis notebook has the complete code to load, prep, train and deploy the model. We chose a small public data set for this demo so as to run the entire process in only few minutes.\n\nFollowing are the high level steps:\n\n1. Create AML Workspace\n2. Acquire and Prepare Data\n3. Automated ML\n4. Deploy Model as webservice\n5. Predictions"],"metadata":{}},{"cell_type":"markdown","source":["## 1. Create cluster (in this lab it is pre-created)\n\nPlease follow the instructions from Microsoft documentation with your customers https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-environment#azure-databricks"],"metadata":{}},{"cell_type":"markdown","source":["## 2. Acquire and Prepare Data\nFor this notebook, we will use the NASA Prognostics Center's Turbo-Fan Failure dataset.  It is located here: https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan"],"metadata":{}},{"cell_type":"markdown","source":["Download and un-zip the data"],"metadata":{}},{"cell_type":"code","source":["import logging\nimport os\nimport random\nimport time\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport numpy as np\nimport pandas as pd"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["# import needed libraries for downloading and unzipping the file\nimport urllib.request\nfrom zipfile import ZipFile"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# download from url\nresponse = urllib.request.urlopen(\"https://ti.arc.nasa.gov/c/6/\")\noutput = open('CMAPSSData.zip', 'wb')    # note the flag:  \"wb\"        \noutput.write(response.read())\noutput.close()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["# unzip files\nzipfile = ZipFile(\"CMAPSSData.zip\")\nzipfile.extract(\"train_FD001.txt\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">4</span><span class=\"ansired\">]: </span>&apos;/databricks/driver/train_FD001.txt&apos;\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["Next we read our data into a Pandas DataFrame.\nNote the headers were not in the space seperated txt file, so we assign them from the ReadMe in the zip file. In pandas we use read_csv with the delimiter option."],"metadata":{}},{"cell_type":"code","source":["df = pd.read_csv(\"train_FD001.txt\", delimiter=\"\\s|\\s\\s\", index_col=False, engine='python', names=['unit','cycle','os1','os2','os3','sm1','sm2','sm3','sm4','sm5','sm6','sm7','sm8','sm9','sm10','sm11','sm12','sm13','sm14','sm15','sm16','sm17','sm18','sm19','sm20','sm21'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"markdown","source":["Take a quick look at the data"],"metadata":{}},{"cell_type":"code","source":["df.head(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>\n   unit  cycle     os1     os2    os3     sm1     sm2      sm3      sm4  \\\n0     1      1 -0.0007 -0.0004  100.0  518.67  641.82  1589.70  1400.60   \n1     1      2  0.0019 -0.0003  100.0  518.67  642.15  1591.82  1403.14   \n2     1      3 -0.0043  0.0003  100.0  518.67  642.35  1587.99  1404.20   \n3     1      4  0.0007  0.0000  100.0  518.67  642.35  1582.79  1401.87   \n4     1      5 -0.0019 -0.0002  100.0  518.67  642.37  1582.85  1406.22   \n\n     sm5   ...       sm12     sm13     sm14    sm15  sm16  sm17  sm18   sm19  \\\n0  14.62   ...     521.66  2388.02  8138.62  8.4195  0.03   392  2388  100.0   \n1  14.62   ...     522.28  2388.07  8131.49  8.4318  0.03   392  2388  100.0   \n2  14.62   ...     522.42  2388.03  8133.23  8.4178  0.03   390  2388  100.0   \n3  14.62   ...     522.86  2388.08  8133.83  8.3682  0.03   392  2388  100.0   \n4  14.62   ...     522.19  2388.04  8133.80  8.4294  0.03   393  2388  100.0   \n\n    sm20     sm21  \n0  39.06  23.4190  \n1  39.00  23.4236  \n2  38.95  23.3442  \n3  38.88  23.3739  \n4  38.90  23.4044  \n\n[5 rows x 26 columns]\n</div>"]}}],"execution_count":15},{"cell_type":"markdown","source":["Our dataset has a number of units in it, with each engine flight listed as a cycle. The cycles count up until the engine fails. What we would like to predict is the no. of cycles until failure. \nSo we need to calculate a new column called RUL, or Remaining Useful Life.  It will be the last cycle value minus each cycle value per unit."],"metadata":{}},{"cell_type":"code","source":["# Assign ground truth\ndef assignrul(dft):\n    maxi = dft['cycle'].max()\n    dft['rul'] = maxi - dft['cycle']\n    return dft\n    \n\ndf_new = df.groupby('unit').apply(assignrul) #derive label column\ndf_new = df_new.drop(['unit'], axis=1) #Remove unit column because it wont help us do prediction"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"code","source":["#download file using https://eastus2.azuredatabricks.net/files/df_new.csv\ndf_new.to_csv(\"/dbfs/FileStore/df_new.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["# put training data into X and Y df\n# removing 1st row to do evaluation later on\nX = df_new.drop(['rul'], axis=1)[2:]\ny = df_new[['rul']][2:]\n\nfeatures = X.columns #derive features"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2)\n\nX_train = pd.DataFrame(X_train, columns=features)\nX_test = pd.DataFrame(X_test, columns=features)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"code","source":["X_train.head(5)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["y_train.head(5)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["X_eval = df_new.drop(['rul'], axis=1)[0:1]\ny_eval = df_new['rul'][0:1]\nprint (X_eval)\nprint (y_eval)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["## 3. Azure Automated ML"],"metadata":{}},{"cell_type":"markdown","source":["Here we utilize Azure's AutoML package to automate the scaling of the sensors, selection of sensors, and automatically train and evaluate many different types of ML models."],"metadata":{}},{"cell_type":"code","source":["import azureml.core\n\n# Check core SDK version number - based on build number of preview/master.\nprint(\"SDK version:\", azureml.core.VERSION)\n\nusername = dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().apply('user').split(\"@\")[0]\nprint(\"Your username is {0}\".format(username))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["%sh  \n/databricks/python/bin/pip freeze > /tmp/python_packages.txt\nls -lrt /tmp/python_packages.txt\ncat /tmp/python_packages.txt"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["from azureml.core.experiment import Experiment\nfrom azureml.core.workspace import Workspace\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.train.automl.run import AutoMLRun"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["![Workspace](https://github.com/parasharshah/automl-handson/raw/master/image1.JPG)"],"metadata":{}},{"cell_type":"markdown","source":["Provide your Machine Learning Workspace credentials to run AutoML. You will need to perform Microsoft's MFA. Please follow the manual auth instructions."],"metadata":{}},{"cell_type":"code","source":["subscription_id = \"<Your SubscriptionId>\" #you should be owner or contributor\nresource_group = \"<Resource group - new or existing>\" #you should be owner or contributor\nworkspace_name = \"<workspace to be created>\" #your workspace name"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["subscription_id = \"ba7979f7-d040-49c9-af1a-7414402bf622\" #you should be owner or contributor\nresource_group = \"automl_ps_newrg\" #you should be owner or contributor\nworkspace_name = \"AutoML_ws_pasha\"              # your workspace name - needs to be unique - can be anything"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["You can have more options when creating Workspace\n\nhttps://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace.workspace?view=azure-ml-py has more options.\n\nFor auth - https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azure-ml.ipynb"],"metadata":{}},{"cell_type":"code","source":["ws = Workspace.get(name = workspace_name,\n                      subscription_id = subscription_id,\n                      resource_group = resource_group)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["# Choose a name for the experiment and specify the project folder.\nexperiment_name = 'automl-predictive-rul'\nproject_folder = './sample_projects/automl-demo-predmain'\n\nexperiment = Experiment(ws, experiment_name)\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace Name'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\noutput['Project Directory'] = project_folder\noutput['Experiment Name'] = experiment.name\npd.set_option('display.max_colwidth', -1)\npd.DataFrame(data = output, index = ['']).T"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["import azureml.dataprep as dprep\nimport uuid\n\nXtrain_dflow = dprep.read_pandas_dataframe(X_train, temp_folder='/dbfs/tmp'+str(uuid.uuid4()))\nytrain_dflow = dprep.read_pandas_dataframe(y_train, temp_folder='/dbfs/tmp'+str(uuid.uuid4()))\nXtest_dflow = dprep.read_pandas_dataframe(X_train, temp_folder='/dbfs/tmp'+str(uuid.uuid4()))\nytest_dflow = dprep.read_pandas_dataframe(y_train, temp_folder='/dbfs/tmp'+str(uuid.uuid4()))"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["ytrain_dflow.get_profile()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["Now we are ready to configure Azure Automated ML.  We provide necessary information on: what we want to predict, what accuracy metric we want to use, how many models we want to try, and many other parameters. Automated ML will also automatically scale the data for us."],"metadata":{}},{"cell_type":"markdown","source":["![Workspace](https://github.com/parasharshah/automl-handson/raw/master/image6b.jpg)"],"metadata":{}},{"cell_type":"markdown","source":["## Configure Automated ML\n\nYou can use these params. All params in Azure Doc - https://docs.microsoft.com/en-us/python/api/azureml-train-automl/azureml.train.automl.automlconfig?view=azure-ml-py\n\n|Property|Description|\n|-|-|\n|**task**|classification or regression or forecasting|\n|**primary_metric**|This is the metric that you want to optimize. Classification supports the following primary metrics: <br><i>accuracy</i><br><i>AUC_weighted</i><br><i>average_precision_score_weighted</i><br><i>norm_macro_recall</i><br><i>precision_score_weighted</i>|\n|**primary_metric**|This is the metric that you want to optimize. Regression supports the following primary metrics: <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>|\n|**iteration_timeout_minutes**|Time limit in minutes for each iteration.|\n|**iterations**|Number of iterations. In each iteration AutoML trains a specific pipeline with the data.|\n|**max_cores_per_iteration**|Default is 1 if not specified else give max cores of your VM. Not every algorithm will use multiple cores.|\n|**n_cross_validations**|Number of cross validation splits. Do not use this when explicit validation set is provided.|\n|**spark_context**|Spark Context object. for Databricks, use spark_context=sc|\n|**max_concurrent_iterations**|Maximum number of iterations to execute in parallel. This should be <= number of worker nodes in your Azure Databricks cluster.|\n|**X**|(sparse) array-like, shape = [n_samples, n_features]. For Azure Databricks, this has to be a dataflow.|\n|**y**|\t(sparse) array-like, shape = [n_samples, ], Multi-class targets. For Azure Databricks, this has to be a dataflow.|\n|**X_valid**|\t(sparse) array-like, shape = [n_samples, n_features]. For Azure Databricks, this has to be a dataflow.|\n|**y_valid**|\t(sparse) array-like, shape = [n_samples, ], Multi-class targets. For Azure Databricks, this has to be a dataflow.|\n|**model_explainability**|\tIndicate to explain each trained pipeline or not. Requires validation set. Set as True or False.|\n|**path**|Relative path to the project folder. AutoML stores configuration files for the experiment under this folder. You can specify a new empty folder.|\n|**preprocess**|set this to True to enable pre-processing of data eg. string to numeric using one-hot encoding. Set as True or False.|\n|**experiment_exit_score**|Target score for experiment. It is associated with the metric. eg. experiment_exit_score=0.995 will exit experiment after that|\n|**enable_early_stopping**|Flag to enble early termination if the score is not improving in the short term. Set as True or False.|"],"metadata":{}},{"cell_type":"code","source":["automl_config = AutoMLConfig(task = 'regression',\n                             debug_log = 'automl_errors_regression.log',\n                             primary_metric = 'r2_score',\n                             iteration_timeout_minutes = 10, #some runs may take 10+ mins hence limiting it for workshop\n                             iterations = 50, #you may change this to a higher number and see what happens                             \n                             verbosity = logging.INFO,\n                             max_cores_per_iteration=4, #each VM has 4 cores\n                             max_concurrent_iterations = 2, #change it based on number of worker nodes\n                             spark_context=sc, #databricks/spark related\n                             #n_cross_validations = 4, #only needed for small datasets and if validation size is not set\n                             X = Xtrain_dflow,\n                             y = ytrain_dflow,\n                             X_valid = Xtest_dflow, #either provide this or use cross validation\n                             y_valid = ytest_dflow, #either provide this or use cross validation\n                             model_explainability = False, #enable only if doing model explain\n                             enable_early_stopping = True,\n                             preprocess=True, #preprocess\n                             path = project_folder)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["Finally we are ready to submit the experiment to Automated ML service. This step can take longer depending on the settings. AutoML will give us updates as models are trained and evaluated by the metric we specified above. The information from each ML model training will be stored in the Experiment section of the Azure ML Workspace in Azure Portal."],"metadata":{}},{"cell_type":"code","source":["local_run = experiment.submit(automl_config, show_output = False) # you may set it to True to see results here but portal experience is better."],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["displayHTML(\"<a href={} target='_blank'>Your experiment in Azure Portal: {}</a>\".format(local_run.get_portal_url(), local_run.id))"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# find the run with the highest accuracy value.\nbest_run, fitted_model = local_run.get_output()\nprint(best_run)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"code","source":["#The fitted_model is a python object and you can read the different properties of the object. The following shows printing hyperparameters for each step in the pipeline.\n\nfrom pprint import pprint\n\ndef print_model(model, prefix=\"\"):\n    for step in model.steps:\n        print(prefix + step[0])\n        if hasattr(step[1], 'estimators') and hasattr(step[1], 'weights'):\n            pprint({'estimators': list(e[0] for e in step[1].estimators), 'weights': step[1].weights})\n            print()\n            for estimator in step[1].estimators:\n                print_model(estimator[1], estimator[0]+ ' - ')\n        else:\n            pprint(step[1].get_params())\n            print()\n            \nprint_model(fitted_model)"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["#from azureml.train.automl.automlexplainer import retrieve_model_explanation\n\n#shap_values, expected_values, overall_summary, overall_imp, per_class_summary, per_class_imp = \\\n#   retrieve_model_explanation(best_run)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["#print(overall_summary)\n#print(overall_imp)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["## 4. Deploy Model"],"metadata":{}},{"cell_type":"markdown","source":["![Workspace](https://github.com/parasharshah/automl-handson/raw/master/image4-automl.jpg)"],"metadata":{}},{"cell_type":"code","source":["# register model in workspace & use the same in your score.py file\ndescription = 'AutoML-RUL-Regression-20190514'\ntags = None\nmodel=local_run.register_model(description=description, tags=tags)\nlocal_run.model_id # Use this id to deploy the model as a web service in Azure. Update score file with the output."],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["After we register the model in our AML Workspace, it should be visible in Azure Portal.\n\nNow we want to deploy the model as a REST API (real time webservice) that we can feed a row or rows of \"X\" data to, and return the predicted 'RUL' value.  To accomplish this, we will build a container image in our AML Workspace and deploy that image as a Container instance in Azure's ACI service.  We will then obtain an IP address where we can submit data and receive back the predicted 'RUL' value.\n\nThere are 3 things we need: \n1. A score.py file that contains the init() and run() functions with instructions on how to load and score with the model. Update the model name in this file.\n2. A mydeployenv.yml file that contains information on the python environment in which the model needs to run\n3. Configurations for our images and our services, using functions provided by AzureML service.\n\nThe cells below help you set these up."],"metadata":{}},{"cell_type":"code","source":["scorefilename = (('score'+str(uuid.uuid4()))[0:10]) + \".py\"\nprint(scorefilename) #change the filename in score file"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["%%writefile scoree9690.py\n# Change the name based on the randomly generated filename\n# Scoring Script will need model id from registered model\nimport json\nimport numpy as np\nimport os\nimport pickle\nimport pandas as pd\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression\n\nfrom azureml.core.model import Model\n\nimport azureml.train.automl\n\ndef init():\n    global model\n    # retreive the path to the model file using the model name\n    model_path = Model.get_model_path('AutoML979330f1fbest') # update this based on previously registered model\n    print(model_path)\n    model = joblib.load(model_path)\n    \n\ndef run(raw_data):\n    # grab and prepare the data\n    #data = (np.array(json.loads(raw_data)['data'])).reshape(1,-1)\n    data = (pd.DataFrame(np.array(json.loads(raw_data)['data']), columns=['cycle', 'os1', 'os2', 'os3', 'sm1', 'sm2', 'sm3', 'sm4', 'sm5', 'sm6', 'sm7', 'sm8', 'sm9', 'sm10', 'sm11', 'sm12', 'sm13', 'sm14', 'sm15', 'sm16', 'sm17', 'sm18', 'sm19', 'sm20', 'sm21']))\n    # make prediction\n    y_hat = model.predict(data)\n    return json.dumps(y_hat.tolist())"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":["condafilename = (('mydeploy'+str(uuid.uuid4()))[0:14]) + \".yml\"\nprint(condafilename) #change the filename in score file"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["from azureml.core.conda_dependencies import CondaDependencies\n\nmyenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn==0.19.1'], pip_packages=['azureml-sdk[automl]'])\n\nconda_env_file_name = condafilename\nmyenv.save_to_file('.', conda_env_file_name)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["from azureml.core.webservice import AciWebservice\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=2, \n                                               memory_gb=5, \n                                               tags={\"data\": \"RUL\",  \"method\" : \"sklearn\"}, \n                                               description='Predict RUL with Azure AutoML')"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["# this will take 10-15 minutes to finish\n\nservice_name = \"rul-pred-demo\" #put your name as suffix - no capital letter/special characters\nruntime = \"python\" \ndriver_file = scorefilename #uses the name generated earlier - do not change it\nmy_conda_file = conda_env_file_name #uses the name generated earlier - do not change it\n\n# image creation\nfrom azureml.core.image import ContainerImage\nmyimage_config = ContainerImage.image_configuration(execution_script = driver_file, \n                                    runtime = runtime, \n                                    conda_file = my_conda_file)\n\n# Webservice creation\nmyservice = AciWebservice.deploy_from_model(\n  workspace=ws, \n  name=service_name,\n  deployment_config = aciconfig,\n  models = [model],\n  image_config = myimage_config\n    )\n\nmyservice.wait_for_deployment(show_output=True)"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"code","source":["print(myservice.scoring_uri)"],"metadata":{},"outputs":[],"execution_count":59},{"cell_type":"code","source":["import requests\nimport json\n\nheaders = {'Content-Type':'application/json'}\n\n#this is same as X_eval\ninput_data = \"{\\\"data\\\": [[1.0, -0.0007, -0.0004, 100.0, 518.67, 641.82, 1589.7, 1400.6, 14.62, 21.61, 554.36, 2388.06, 9046.19, 1.3, 47.47, 521.66, 2388.02, 8138.62, 8.4195, 0.03, 392.0, 2388.0, 100.0, 39.06, 23.419]]}\"\n\n#this is same as y_eval\ntestlabel = '191'\n\nresp = requests.post(myservice.scoring_uri, input_data, headers=headers)\n\nprint(\"POST to url\", myservice.scoring_uri)\nprint(\"input data:\", input_data)\nprint(\"label:\", testlabel)\nprint(\"prediction:\", resp.text)"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["To avoid any run-away Azure costs, we always delete un-necessary services when we are done."],"metadata":{}},{"cell_type":"code","source":["myservice.delete()"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["![Workspace](https://github.com/parasharshah/automl-handson/raw/master/image5.JPG)"],"metadata":{}},{"cell_type":"code","source":["from azureml.core.image import Image\nmyimage = Image(workspace=ws, name=service_name) # image is based on the service name provided earlier for ACI"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"code","source":["#create AKS compute\n#it may take 20-25 minutes to create a new cluster\n\nfrom azureml.core.compute import AksCompute, ComputeTarget\n\n# Use the default configuration (can also provide parameters to customize)\nprov_config = AksCompute.provisioning_configuration()\n\naks_name = 'ps-aks-demo' \n\n# Create the cluster\naks_target = ComputeTarget.create(workspace = ws, \n                                  name = aks_name, \n                                  provisioning_configuration = prov_config)\n\naks_target.wait_for_completion(show_output = True)\n\nprint(aks_target.provisioning_state)\nprint(aks_target.provisioning_errors)"],"metadata":{},"outputs":[],"execution_count":65},{"cell_type":"code","source":["from azureml.core.webservice import Webservice, AksWebservice\nfrom azureml.core.image import ContainerImage\n\n#Set the web service configuration (using default here with app insights)\naks_config = AksWebservice.deploy_configuration(enable_app_insights=True)\n\n#unique service name\nservice_name_aks ='ps-aks-service-demo'\n\n# Webservice creation using single command, there is a variant to use image directly as well.\naks_service = Webservice.deploy_from_image(\n  workspace=ws, \n  name=service_name_aks,\n  deployment_config = aks_config,\n  image = myimage,\n  deployment_target = aks_target\n    )\n\naks_service.wait_for_deployment(show_output=True)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["#for using the Web HTTP API \nprint(aks_service.scoring_uri)\nprint(aks_service.get_keys())"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["import requests\nimport json\n\n\ninput_data = \"{\\\"data\\\": [[1.0, -0.0007, -0.0004, 100.0, 518.67, 641.82, 1589.7, 1400.6, 14.62, 21.61, 554.36, 2388.06, 9046.19, 1.3, 47.47, 521.66, 2388.02, 8138.62, 8.4195, 0.03, 392.0, 2388.0, 100.0, 39.06, 23.419]]}\"\ntestlabel = '191'\n\nheaders = {'Content-Type':'application/json'}\n\n# for AKS deployment you'd need to the service key in the header as well\napi_key = 'CZXzrbMARTITqh4SxngyBXalkQHUzDEE' #change this value based on above api key value\nheaders = {'Content-Type':'application/json',  'Authorization':('Bearer '+ api_key)} \n\nresp = requests.post(aks_service.scoring_uri, input_data, headers=headers)\n\nprint(\"POST to url\", aks_service.scoring_uri)\nprint(\"input data:\", input_data)\nprint(\"label:\", testlabel)\nprint(\"prediction:\", resp.text)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["## 5. Conclusions"],"metadata":{}},{"cell_type":"markdown","source":["We have executed an end-to-end Azure ML Service project with a real life example. We started with a problem at hand, created an Azure ML Workspace, downloaded a predictive maintenance dataset, processed the data, train a sophisticated model with Azure Automated ML, and deployed that model quickly and easily to production level service with AKS using Azure's Machine Learning service."],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python (env1231)","language":"python","name":"env1231"},"name":"07.Predictive Maint demo with ADB v2","notebookId":1073940528116570},"nbformat":4,"nbformat_minor":0}
